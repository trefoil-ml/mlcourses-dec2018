---
title: "Train a GBM model"
author: "Hicham Zmarrou"
date: "`r Sys.Date()`"
output:
  html_notebook:
    highlight: pygments
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
  word_document:
    toc: yes
---


## Train a GBM model


Here you will use the `gbm()` function to train a GBM classifier to predict loan default. You will train a 10,000-tree GBM on the credit_train dataset, which is pre-loaded into your workspace.

Using such a large number of trees (10,000) is probably not optimal for a GBM model, but we will build more trees than we need and then select the optimal number of trees based on early performance-based stopping. The best GBM model will likely contain fewer trees than we started with.

For binary classification, `gbm()` requires the response to be encoded as 0/1 (numeric), so we will have to convert from a "no/yes" factor to a 0/1 numeric response column.



Also, the the `gbm()` function requires the user to specify a distribution argument. For a binary classification problem, you should set distribution = "bernoulli". The Bernoulli distribution models a binary response.


``` 

library(gbm)
# Execute the tuning process
# Convert "yes" to 1, "no" to 0
credit_train$default <- ifelse(credit_train$default == "yes", ___, ___)

# Train a 10000-tree GBM model
set.seed(1)
credit_model <- gbm(formula = default ~ ., 
                    distribution = ___, 
                    data = ___,
                    n.trees = ___)
                    
# Print the model object                    
print(___)

# summary() prints variable importance
summary(___)

``` 



## Prediction using a GBM model

The `gbm` package uses a `predict()` function to generate predictions from a model, similar to many other machine learning packages in R. When you see a function like  `predict()` that works on many different types of input (a GBM model, a RF model, a GLM model, etc), that indicates that `predict()` is an "alias" for a GBM-specific version of that function. The GBM specific version of that function is `predict.gbm()`, but for convenience sake, we can just use `predict()` (either works).

One thing that's particular to the predict.gbm() however, is that you need to specify the number of trees used in the prediction. There is no default, so you have to specify this manually. For now, we can use the same number of trees that we specified when training the model, which is 10,000 (though this may not be the optimal number to use).

Another argument that you can specify is type, which is only relevant to Bernoulli and Poisson distributed outcomes. When using Bernoulli loss, the returned value is on the log odds scale by default and for Poisson, it's on the log scale. If instead you specify `type = "response"`, then gbm converts the predicted values back to the same scale as the outcome. This will convert the predicted values into probabilities for Bernoulli and expected counts for Poisson.


* Generate predictions on the test set, using 10,000 trees.
* Generate predictions on the test set using type = "response" and 10,000 trees.
* Compare the ranges of the two sets of predictions.



```
# Since we converted the training response col, let's also convert the test response col
credit_test$default <- ifelse(credit_test$default == "yes", 1, 0)

# Generate predictions on the test set
preds1 <- predict(object = ___, 
                  newdata = ___,
                  n.trees = ___)

# Generate predictions on the test set (scale to response)
preds2 <- predict(object = ___, 
                  newdata = ___,
                  n.trees = ___,
                  type = "response")

# Compare the range of the two sets of predictions
range(preds1)
range(preds2)

```

Evaluate test set AUC
Compute test set AUC of the GBM model for the two sets of predictions. We will notice that they are the same value. That's because AUC is a rank-based metric, so changing the actual values does not change the value of the AUC.

However, if we were to use a scale-aware metric like RMSE to evaluate performance, we would want to make sure we converted the predictions back to the original scale of the response.


* Compute AUC of the predictions.
* Compute AUC of the predictions (scaled to response).
* Notice that the AUC is the same!


```
# Generate the test set AUCs using the two sets of preditions & compare
auc(actual = credit_test$default, predicted = ___)  #default
auc(actual = credit_test$default, predicted = ___)  #rescaled

```

## Early stopping in GBMs

Use the gbm.perf() function to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error. When you set out to train a large number of trees in a GBM (such as 10,000) and you use a validation method to determine an earlier (smaller) number of trees, then that's called "early stopping". The term "early stopping" is not unique to GBMs, but can describe auto-tuning the number of iterations in an iterative learning algorithm.



* The credit_model object is loaded in the workspace.

*Use the gbm.perf() function with the "OOB" method to get the optimal number of trees based on the OOB error and store that number as ntree_opt_oob.

* Train a new GBM model, this time with cross-validation, so we can get a cross-validated estimate of the optimal number of trees.

* Lastly, use the gbm.perf() function with the "cv" method to get the optimal number of trees based on the CV error and store that number as ntree_opt_cv.

* Compare the two numbers.

```
# Optimal ntree estimate based on OOB
ntree_opt_oob <- gbm.perf(object = ___, 
                          method = ___, 
                          oobag.curve = TRUE)

# Train a CV GBM model
set.seed(1)
credit_model_cv <- gbm(formula = default ~ ., 
                       distribution = "bernoulli", 
                       data = credit_train,
                       n.trees = 10000,
                       cv.folds = 2)

# Optimal ntree estimate based on CV
ntree_opt_cv <- gbm.perf(object = ___, 
                         method = ___)
 
# Compare the estimates                         
print(paste0("Optimal n.trees (OOB Estimate): ", ntree_opt_oob))                         
print(paste0("Optimal n.trees (CV Estimate): ", ntree_opt_cv))


```

## OOB vs CV-based early stopping

In the previous exercise, we used OOB error and cross-validated error to estimate the optimal number of trees in the GBM. These are two different ways to estimate the optimal number of trees, so in this exercise we will compare the performance of the models on a test set. We can use the same model object to make both of these estimates since the `predict.gbm()` function allows you to use any subset of the total number of trees (in our case, the total number is 10,000).



* The ntree_opt_oob and ntree_opt_cv objects from the previous exercise (each storing an "optimal" value for n.trees) are loaded in the workspace.

* Using the credit_model loaded in the workspace, generate two sets of predictions:

* One using the OOB estimate of n.trees: 3,233 (stored in ntree_opt_oob)
And the other using the CV estimate of n.trees: 7,889 (stored in ntree_opt_cv)


```
# Generate predictions on the test set using ntree_opt_oob number of trees
preds1 <- predict(object = ___, 
                  newdata = ___,
                  n.trees = ntree_opt_oob)
                  
# Generate predictions on the test set using ntree_opt_cv number of trees
preds2 <- predict(object = ___, 
                  newdata = ___,
                  n.trees = ___)   

# Generate the test set AUCs using the two sets of preditions & compare
auc1 <- auc(actual = credit_test$default, predicted = preds1)  #OOB
auc2 <- auc(actual = credit_test$default, predicted = ___)  #CV 

# Compare AUC 
print(paste0("Test set AUC (OOB): ", auc1))                         
print(paste0("Test set AUC (CV): ", auc2))

```

## Plot & compare ROC curves

We conclude this course by plotting the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values.

The more "up and to the left" the ROC curve of a model is, the better the model. The AUC performance metric is literally the "Area Under the ROC Curve", so the greater the area under this curve, the higher the AUC, and the better-performing the model is.

The ROCR package can plot multiple ROC curves on the same plot if you plot several sets of predictions as a list.

* The prediction() function takes as input a list of prediction vectors (one per model) and a corresponding list of true values (one per model, though in our case the models were all evaluated on the same test set so they all have the same set of true values). The prediction() function returns a "prediction" object which is then passed to the performance() function.

* The performance() function generates the data necessary to plot the curve from the "prediction" object. For the ROC curve, you will also pass along two measures, "tpr" and "fpr".

* Once you have the "performance" object, you can plot the ROC curves using the plot() method. We will add some color to the curves and a legend so we can tell which curves belong to which algorithm.

```
# List of predictions
preds_list <- list(dt_preds, bag_preds, rf_preds, gbm_preds)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(credit_test$default), m)

# Plot the ROC curves
pred <- prediction(preds_list, ___)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)

```




## Solutions 

###  Train a GBM model
```{r}
# Convertit "yes" to 1, "no" to 0
#credit_train$default <- ifelse(credit_train$default == "Yes", 1, 0)
#credit_test$default<- ifelse(credit_test$default == "Yes", 1, 0)
# Train avec 10000-tree 
set.seed(12)
credit_model <- gbm(formula = default ~ ., 
                    distribution = "bernoulli", # pour deux class
                    data = credit_train,
                    n.trees = 800)

```

```{r}
# Print  model                    
print(credit_model)
```



```{r echo=TRUE}
# summary() prints variable importance
summary(credit_model)
```



### Prediction using a GBM model


```{r}
library(ModelMetrics)
# Prediction 
preds1 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = 500) # Nombre d'arbre à utiliser dans la prediction

# Prediction (avec response)
preds2 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = 500,
                  type = "response")

# Compare compare les intervalles de prediction
range(preds1)
## 
range(preds2)

auc(actual = credit_test$default, predicted = preds1)  #default
## 
auc(actual = credit_test$default, predicted = preds2)  #rescaled  
## 


```

## Early stopping in GBMs

```{r}

# le ntree optimal pour   OOB
ntree_opt_oob <- gbm.perf(object = credit_model, 
                          method = "OOB", 
                          oobag.curve = TRUE)

```

```{r}
# Train avec Cross validation GBM model
set.seed(1)
credit_model_cv <- gbm(formula = default ~ ., 
                       distribution = "bernoulli", 
                       data = credit_train,
                       n.trees = 10000,
                       cv.folds = 2)

# ntree optimal  pour  CV
ntree_opt_cv <- gbm.perf(object = credit_model_cv, 
                         method = "cv")
```





```{r}
# compare estimation                       
print(paste0("Optimal n.trees (OOB Estimate): ", ntree_opt_oob))                         
```



```{r}
print(paste0("Optimal n.trees (CV Estimate): ", ntree_opt_cv))  

```


```{r}
# prediction sur credit_test apres de  ntree_opt_oob nombre de trees
preds1 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = ntree_opt_oob)
                  
# prediction sur credit_test apres de  ntree_opt_cv nombre de trees
preds2 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees =ntree_opt_cv )   
```



### Performance 

```{r}
# prediction sur credit_test apres de  ntree_opt_oob nombre de trees
preds1 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees = ntree_opt_oob, type="response")
                  
# prediction sur credit_test apres de  ntree_opt_cv nombre de trees
preds2 <- predict(object = credit_model, 
                  newdata = credit_test,
                  n.trees =ntree_opt_cv )   

# Generate the test set AUCs using the two sets of preditions & compare
auc1 <- auc(actual = credit_test$default, predicted = preds1)  #OOB
auc2 <- auc(actual = credit_test$default, predicted = preds2)  #CV 

# Compare AUC 
print(paste0("Test set AUC (OOB): ", auc1))      
```





```{r}
print(paste0("Test set AUC (CV): ", auc2)) 
```

```{r}
# List of predictions
preds_list <- list(preds1,preds2)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(credit_test$default), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("OOB", "CV"),
       fill = 1:m)
```



