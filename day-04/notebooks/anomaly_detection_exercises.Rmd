---
title: "Lab: anomaly detection with Twitter AnomalyDetection Package"
author: "Hicham Zmarrou"
date: "`r Sys.Date()`"
output:
  html_notebook:
    highlight: pygments
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
  word_document:
    toc: yes
---
___________________________________________________________________________________________________________


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,
                      digits = 3)
```


Load the packages `AnomalyDetection` and `wikipediatrend`  The `wikipediatrend` package provides convenience
access to daily page view counts (Wikipedia article traffic statistics) stored at [toollabs:pageviews](https://tools.wmflabs.org/pageviews). If you want to know how often an article has been viewed over time and work with the data from within R, this package is for you. Have a look at page counts for Flue, Ebola, Climate Change or Millennium Development Goals and maybe build a model or two. 


```{r}
library(AnomalyDetection)
library(wikipediatrend)
library(dygraphs)
library(xts)
library(lubridate)
```

Example 

```{r}
#download Climate_Change page view history
page_views <- wp_trend("Climate_Change")
#format data
page_viewsReady = data.frame(time=as.POSIXct(page_views$date), count=page_views$views)

#search / display anomalies
res = AnomalyDetectionTs(page_viewsReady, max_anoms=0.01, direction='both', plot=TRUE)
res$plot
```

1. Play with a number of interesting subjects from Wikipedia and display anonmalies? 




