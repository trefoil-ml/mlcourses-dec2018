abline(h = 0.5, lty = 2)
# Compute DBSCAN using fpc::dbscan() and dbscan::dbscan(). Make sure that the 2 packages are installed:
set.seed(123)
# fpc package
res.fpc <- fpc::dbscan(iris, eps = 0.4, MinPts = 4)
# dbscan package
res.db <- dbscan::dbscan(iris, 0.4, 4)
library(AnomalyDetection)
library(wikipediatrend)
library(dygraphs)
library(xts)
library(lubridate)
#download Climate_Change page view history
page_views <- wp_trend("Climate_Change")
#format data
page_viewsReady = data.frame(time=as.POSIXct(page_views$date), count=page_views$views)
#search / display anomalies
res = AnomalyDetectionTs(page_viewsReady, max_anoms=0.01, direction='both', plot=TRUE)
res$plot
library(prophet)
??prophet
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', digits = 3,  echo=TRUE, results=TRUE, warning=FALSE, comment="", message=FALSE)
library(prophet)
df <- read.csv("example_wp_log_peyton_manning.csv")
m <- prophet(df)
future <- make_future_dataframe(m, periods = 365)
tail(future,3)
forecast <- predict(m, future)
tail(forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')],3)
plot(m, forecast)
prophet_plot_components(m, forecast)
dyplot.prophet(m, forecast)
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,digits = 3)
holidays <- filter(bikes, holiday == 1) %>%
select(datetime) %>%
distinct()
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,digits = 3)
# Libraries
library(tidyverse)
library(lubridate)
library(prophet)
library(forecast)
# Read data
bikes <- read_csv('./data/bikes.csv') %>%
mutate(datetime = date(datetime))
head(bikes,5)
# Separate data
train <- bikes %>% select(datetime, count) %>%
filter(datetime < as.Date("2012-01-01")) %>%
group_by(datetime) %>%
summarise(y = sum(count))
names(train) <- c('ds', 'y')
valid <- bikes %>% select(datetime, count) %>%
filter(datetime >= as.Date("2012-01-01") & datetime < as.Date("2012-07-01")) %>%
group_by(datetime) %>%
summarise(y = sum(count))
names(valid) <- c('ds', 'y')
test <- bikes %>% select(datetime, count) %>%
filter(datetime >= as.Date("2012-07-01")) %>%
group_by(datetime) %>%
summarise(y = sum(count))
names(test) <- c('ds', 'y')
holidays <- filter(bikes, holiday == 1) %>%
select(datetime) %>%
distinct()
holidays$holiday = c('Martin Luther King', 'Emancipation Day', 'Independence Day',
'Labor Day', 'Columbus Day', 'Veterans Day', 'New Year',
'Martin Luther King', 'Emancipation Day', 'Independence Day',
'Labor Day', 'Columbus Day', 'Veterans Day')
names(holidays) <- c('ds', 'holiday')
holidays <- filter(bikes, holiday == 1) %>%
select(datetime) %>%
distinct()
holidays$holiday = c('Martin Luther King', 'Emancipation Day', 'Independence Day',
'Labor Day', 'Columbus Day', 'Veterans Day', 'New Year',
'Martin Luther King', 'Emancipation Day', 'Independence Day',
'Labor Day', 'Columbus Day', 'Veterans Day')
names(holidays) <- c('ds', 'holiday')
holidays
seed(2003)
se.seed(2003)
set.seed(2003)
rm(list =ls()O)
rm(list =ls())
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,
digits = 3)
Sys.setenv(TZ="Europe/Berlin")
options(tz="Europe/Berlin")
set.seed(2003)
library(caret)
bank <- read.table("./data/bank-additional-full.csv", header = TRUE, sep = ";", stringsAsFactors = FALSE)
bank <- bank[,c('y','duration','campaign','pdays','previous','euribor3m')]
# rename target class value 'yes' for better interpretation
bank$y[bank$y=='yes'] <- 'term.deposit'
#explore data
str(bank)
# prepare data for training and train models
test_size = 0.3
train_index =  sample(seq(1, nrow(bank)),size = (1 - test_size)*nrow(bank) ,replace = F)
train <- bank[train_index,]
test <- bank[-train_index,]
# setting caret cross validation, here tuned for speed (not accuracy!)
fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)
logitmod   <- caret::train(y ~.,data = train, method = "glmnet",family="binomial",trControl = fitControl)
# logitmod  <- glm(factor(y) ~.,data = train, family = "binomial")
# random forest using randomForerst package, here tuned for speed (not accuracy!)
#rfmod  = caret::train(y ~.,data = train, method = "rf", trControl = fitControl, tuneGrid = expand.grid(.mtry = 2))
# Gradient boosting machine using gbm package, here tuned for speed
#gbmod  = caret::train(y ~.,data = train, method = "gbm",distribution="bernoulli", verbose=F,trControl = fitControl, tuneGrid=data.frame(.n.trees= 1000, .shrinkage=0.01,.interaction.depth=1,.n.minobsinnode=1))
logiPred = predict(logitmod, test, na.action = na.pass)
logiPred_prob = predict(logitmod, test, type = "prob")
postResample(logiPred, factor(test$y))
df <-  cbind(test$y,logiPred_prob)
predictions <- df[,c(1,3)]
names(predictions) <- c("deposit", "pred")
(confmat <- caret::confusionMatrix(logiPred, factor(test$y), positive= "term.deposit", mode = "everything"))
tbl <- table(logiPred, factor(test$y))
#precision(tbl)
(prec <- caret::precision(data = logiPred, reference = factor(test$y), relevant = "term.deposit"))
#logiPred, factor(test$y)
#recall(tbl)
(rec <- caret::recall(data = logiPred, reference = factor(test$y), relevant = "term.deposit"))
(fscore <- (2*prec*rec)/(prec+rec))
setwd("C:/Users/hzmarrou/Downloads/trefoil-ml-courses/day-05/notebooks")
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0, 1, 2)
library(pROC)
resRoc <- pROC::roc(predictions$deposit, predictions$pred)
plot(resRoc, legacy.axes = TRUE)
auc(predictions$deposit, predictions$pred)
library(InformationValue)
# ks_stat(as.numeric(factor(test$y)),as.numeric(logiPred))
(ksstat <- ks_stat(factor(test$y),logiPred, returnKSTable = T))
# ks_plot(factor(test$y),logiPred)
ks_plot(factor(test$y),logiPred)
logiPred
# ks_plot(factor(test$y),logiPred)
ks_plot(factor(test$y),1-logiPred)
# setting caret cross validation, here tuned for speed (not accuracy!)
fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)
# Fit a logistic model
logitmod  = caret::train(y ~.,data = train, method = "glmnet",family="binomial",trControl = fitControl)
# random forest using randomForerst package, here tuned for speed (not accuracy!)
rfmod  = caret::train(y ~.,data = train, method = "rf", trControl = fitControl, tuneGrid = expand.grid(.mtry = 2))
# Gradient boosting machine using gbm package, here tuned for speed
gbmod  = caret::train(y ~.,data = train, method = "gbm",distribution="bernoulli", verbose=F,trControl = fitControl, tuneGrid=data.frame(.n.trees= 1000, .shrinkage=0.01,.interaction.depth=1,.n.minobsinnode=1))
881+513
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,
digits = 3)
Sys.setenv(TZ="Europe/Berlin")
options(tz="Europe/Berlin")
set.seed(2003)
library(caret)
bank <- read.table("./data/bank-additional-full.csv", header = TRUE, sep = ";", stringsAsFactors = FALSE)
bank <- bank[,c('y','duration','campaign','pdays','previous','euribor3m')]
# rename target class value 'yes' for better interpretation
bank$y[bank$y=='yes'] <- 'term.deposit'
#explore data
str(bank)
# prepare data for training and train models
test_size = 0.3
train_index =  sample(seq(1, nrow(bank)),size = (1 - test_size)*nrow(bank) ,replace = F)
train <- bank[train_index,]
test <- bank[-train_index,]
# setting caret cross validation, here tuned for speed (not accuracy!)
fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)
logitmod   <- caret::train(y ~.,data = train, method = "glmnet",family="binomial",trControl = fitControl)
# logitmod  <- glm(factor(y) ~.,data = train, family = "binomial")
# random forest using randomForerst package, here tuned for speed (not accuracy!)
#rfmod  = caret::train(y ~.,data = train, method = "rf", trControl = fitControl, tuneGrid = expand.grid(.mtry = 2))
# Gradient boosting machine using gbm package, here tuned for speed
#gbmod  = caret::train(y ~.,data = train, method = "gbm",distribution="bernoulli", verbose=F,trControl = fitControl, tuneGrid=data.frame(.n.trees= 1000, .shrinkage=0.01,.interaction.depth=1,.n.minobsinnode=1))
logiPred = predict(logitmod, test, na.action = na.pass)
logiPred_prob = predict(logitmod, test, type = "prob")
postResample(logiPred, factor(test$y))
df <-  cbind(test$y,logiPred_prob)
predictions <- df[,c(1,3)]
names(predictions) <- c("deposit", "pred")
(confmat <- caret::confusionMatrix(logiPred, factor(test$y), positive= "term.deposit", mode = "everything"))
tbl <- table(logiPred, factor(test$y))
knitr::include_graphics("img/false_pos_pregnant.png")
#precision(tbl)
(prec <- caret::precision(data = logiPred, reference = factor(test$y), relevant = "term.deposit"))
#logiPred, factor(test$y)
#recall(tbl)
(rec <- caret::recall(data = logiPred, reference = factor(test$y), relevant = "term.deposit"))
(fscore <- (2*prec*rec)/(prec+rec))
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0, 1, 2)
library(pROC)
resRoc <- pROC::roc(predictions$deposit, predictions$pred)
plot(resRoc, legacy.axes = TRUE)
auc(predictions$deposit, predictions$pred)
library(InformationValue)
# ks_stat(as.numeric(factor(test$y)),as.numeric(logiPred))
(ksstat <- ks_stat(factor(test$y),logiPred, returnKSTable = T))
# ks_plot(factor(test$y),logiPred)
knitr::include_graphics("img/roc_perf01.png")
knitr::include_graphics("img/model_per01.png")
knitr::include_graphics("img/model_per02.png")
# setting caret cross validation, here tuned for speed (not accuracy!)
fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)
# Fit a logistic model
logitmod  = caret::train(y ~.,data = train, method = "glmnet",family="binomial",trControl = fitControl)
# random forest using randomForerst package, here tuned for speed (not accuracy!)
rfmod  = caret::train(y ~.,data = train, method = "rf", trControl = fitControl, tuneGrid = expand.grid(.mtry = 2))
# Gradient boosting machine using gbm package, here tuned for speed
gbmod  = caret::train(y ~.,data = train, method = "gbm",distribution="bernoulli", verbose=F,trControl = fitControl, tuneGrid=data.frame(.n.trees= 1000, .shrinkage=0.01,.interaction.depth=1,.n.minobsinnode=1))
library(modelplotr)
# transform datasets and model objects into scored data and calculate deciles
prepare_scores_and_deciles(datasets=list("train","test"),
dataset_labels = list("train data","test data"),
models = list("rfmod","logitmod","gbmod"),
model_labels = list("Random forest","Logitic regression","Boosting gradient machine"),
target_column="y")
# transform data generated with prepare_scores_and_deciles into aggregated data for chosen plotting scope
# plotting_scope(select_model_label = 'Gradient boosting machine',select_dataset_label = 'test data')
plotting_scope(select_model_label = 'Boosting gradient machine',select_dataset_label = 'test data')
#
# plot the cumulative gains plot
plot_cumgains()
# plot the cumulative gains plot and annotate the plot at decile = 2
plot_cumgains(highlight_decile = 2)
knitr::include_graphics("img/model_per03.png")
# plot the cumulative lift plot and annotate the plot at decile = 2
plot_cumlift(highlight_decile = 1)
knitr::include_graphics("img/model_per04.png")
# plot the response plot and annotate the plot at decile = 1
plot_response(highlight_decile = 1)
knitr::include_graphics("img/model_per05.png")
# plot the cumulative response plot and annotate the plot at decile = 3
plot_cumresponse(highlight_decile = 3)
# plot all four evaluation plots and save to file
plot_all(save_fig = TRUE,save_fig_filename = 'Selection model Term Deposits')
##
# plot the cumulative gains plot and annotate the plot at decile = 2
plot_cumgains(highlight_decile = 5)
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,
digits = 3)
Sys.setenv(TZ="Europe/Berlin"
set.seed(2003)
decisionplot <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
set.seed(100)
data(iris)
# Three classes
x <- iris[1:150, c("Sepal.Length", "Sepal.Width", "Species")]
# Easier to separate
#x <- iris[1:150, c("Petal.Length", "Petal.Width", "Species")]
head(x)
plot(x[,1:2], col = x[,3])
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
model <- knn3(Species ~ ., data=x, k = 1)
decisionplot(model, x, class = "Species", main = "kNN (1)")
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,
digits = 3)
Sys.setenv(TZ="Europe/Berlin")
set.seed(2003)
decisionplot <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
set.seed(100)
data(iris)
# Three classes
x <- iris[1:150, c("Sepal.Length", "Sepal.Width", "Species")]
head(x)
plot(x[,1:2], col = x[,3])
library(caret)
model <- knn3(Species ~ ., data=x, k = 1)
decisionplot(model, x, class = "Species", main = "kNN (1)")
model <- knn3(Species ~ ., data=x, k = 10)
decisionplot(model, x, class = "Species", main = "kNN (10)")
library(e1071)
model <- naiveBayes(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "naive Bayes")
library(MASS)
model <- lda(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "LDA")
model <- glm(Species ~., data = x, family=binomial(link='logit'))
## Warning: glm.fit: algorithm did not converge
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
class(model) <- c("lr", class(model))
predict.lr <- function(object, newdata, ...)
predict.glm(object, newdata, type = "response") > .5
decisionplot(model, x, class = "Species", main = "Logistic Regression")
library("rpart")
model <- rpart(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "CART")
model <- rpart(Species ~ ., data=x,
control = rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class = "Species", main = "CART (overfitting)")
library(C50)
model <- C5.0(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "C5.0")
library(randomForest)
model <- randomForest(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "Random Forest")
# SVM
library(e1071)
model <- svm(Species ~ ., data=x, kernel="linear")
decisionplot(model, x, class = "Species", main = "SVD (linear)")
model <- svm(Species ~ ., data=x, kernel = "radial")
decisionplot(model, x, class = "Species", main = "SVD (radial)")
model <- svm(Species ~ ., data=x, kernel = "polynomial")
decisrionplot(model, x, class = "Species", main = "SVD (polynomial)")
model <- svm(Species ~ ., data=x, kernel = "polynomial")
decisionplot(model, x, class = "Species", main = "SVD (polynomial)")
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,
digits = 3)
Sys.setenv(TZ="Europe/Berlin")
set.seed(2003)
decisionplot <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
set.seed(100)
data(iris)
# Three classes
x <- iris[1:150, c("Sepal.Length", "Sepal.Width", "Species")]
head(x)
plot(x[,1:2], col = x[,3])
library(caret)
model <- knn3(Species ~ ., data=x, k = 1)
decisionplot(model, x, class = "Species", main = "kNN (1)")
model <- knn3(Species ~ ., data=x, k = 10)
decisionplot(model, x, class = "Species", main = "kNN (10)")
library(e1071)
model <- naiveBayes(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "naive Bayes")
library(MASS)
model <- lda(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "LDA")
model <- glm(Species ~., data = x, family=binomial(link='logit'))
## Warning: glm.fit: algorithm did not converge
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
class(model) <- c("lr", class(model))
predict.lr <- function(object, newdata, ...)
predict.glm(object, newdata, type = "response") > .5
decisionplot(model, x, class = "Species", main = "Logistic Regression")
library("rpart")
model <- rpart(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "CART")
model <- rpart(Species ~ ., data=x,
control = rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class = "Species", main = "CART (overfitting)")
library(C50)
model <- C5.0(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "C5.0")
library(randomForest)
model <- randomForest(Species ~ ., data=x)
decisionplot(model, x, class = "Species", main = "Random Forest")
# SVM
library(e1071)
model <- svm(Species ~ ., data=x, kernel="linear")
decisionplot(model, x, class = "Species", main = "SVD (linear)")
model <- svm(Species ~ ., data=x, kernel = "radial")
decisionplot(model, x, class = "Species", main = "SVD (radial)")
model <- svm(Species ~ ., data=x, kernel = "polynomial")
decisionplot(model, x, class = "Species", main = "SVD (polynomial)")
model <- svm(Species ~ ., data=x, kernel = "sigmoid")
decisionplot(model, x, class = "Species", main = "SVD (sigmoid)")
library(nnet)
model <- nnet(Species ~ ., data=x, size = 1, maxit = 1000, trace = FALSE)
decisionplot(model, x, class = "Species", main = "NN (1)")
model <- nnet(Species ~ ., data=x, size = 2, maxit = 1000, trace = FALSE)
decisionplot(model, x, class = "Species", main = "NN (2)")
model <- nnet(Species ~ ., data=x, size = 4, maxit = 1000, trace = FALSE)
decisionplot(model, x, class = "Species", main = "NN (4)")
model <- nnet(Species ~ ., data=x, size = 10, maxit = 1000, trace = FALSE)
decisionplot(model, x, class = "Species", main = "NN")
set.seed(1000)
library(mlbench)
x <- mlbench.circle(100)
x <- cbind(as.data.frame(x$x), factor(x$classes))
colnames(x) <- c("x", "y", "class")
plot(x[,1:2], col = x[,3])
library(caret)
model <- knn3(class ~ ., data=x, k = 1)
decisionplot(model, x, class = "class", main = "kNN (1)")
model <- knn3(class ~ ., data=x, k = 10)
decisionplot(model, x, class = "class", main = "kNN (10)")
library(e1071)
model <- naiveBayes(class ~ ., data=x)
decisionplot(model, x, class = "class", main = "naive Bayes")
library(MASS)
model <- lda(class ~ ., data=x)
decisionplot(model, x, class = "class", main = "LDA")
model <- glm(class ~., data = x, family=binomial(link='logit'))
class(model) <- c("lr", class(model))
predict.lr <- function(object, newdata, ...)
predict.glm(object, newdata, type = "response") > .5
decisionplot(model, x, class = "class", main = "Logistic Regression")
library("rpart")
model <- rpart(class ~ ., data=x)
decisionplot(model, x, class = "class", main = "CART")
model <- rpart(class ~ ., data=x,
control = rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class = "class", main = "CART (overfitting)")
library(C50)
model <- C5.0(class ~ ., data=x)
decisionplot(model, x, class = "class", main = "C5.0")
library(randomForest)
model <- randomForest(class ~ ., data=x)
decisionplot(model, x, class = "class", main = "Random Forest")
library(e1071)
model <- svm(class ~ ., data=x, kernel="linear")
decisionplot(model, x, class = "class", main = "SVD (linear)")
model <- svm(class ~ ., data=x, kernel = "radial")
decisionplot(model, x, class = "class", main = "SVD (radial)")
model <- svm(class ~ ., data=x, kernel = "polynomial")
decisionplot(model, x, class = "class", main = "SVD (polynomial)")
model <- svm(class ~ ., data=x, kernel = "sigmoid")
decisionplot(model, x, class = "class", main = "SVD (sigmoid)")
library(nnet)
model <- nnet(class ~ ., data=x, size = 1, maxit = 1000, trace = FALSE)
decisionplot(model, x, class = "class", main = "NN (1)")
model <- nnet(class ~ ., data=x, size = 2, maxit = 1000, trace = FALSE)
decisionplot(model, x, class = "class", main = "NN (2)")
model <- nnet(class ~ ., data=x, size = 4, maxit = 10000, trace = FALSE)
decisionplot(model, x, class = "class", main = "NN (4)")
model <- nnet(class ~ ., data=x, size = 10, maxit = 10000, trace = FALSE)
decisionplot(model, x, class = "class", main = "NN (10)")
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0.5, 1, 2)
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0.5, 1, 3)
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0.8, 1, 3)
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0.5, 1, 3)
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0.5, 1, 3)
knitr::include_graphics("img/roc_perf01.png")
