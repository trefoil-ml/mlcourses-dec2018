---
title: Evaluation metrics for Machine Learning models
author: Hicham Zmarrou
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---


<!-- --- -->
<!--  title: "Evaluation metrics for Machine Learning models" -->
<!--  author: "Hicham Zmarrou" -->
<!--  date: "`r Sys.Date()`" -->
<!--  output: -->
<!--    html_notebook: -->
<!--      highlight: espresso -->
<!--      number_sections: no -->
<!--      theme: readable  -->
<!--      toc: yes -->
<!--      toc_float: yes -->
<!--    html_document: -->
<!--      df_print: paged -->
<!--      toc: yes -->
<!--    word_document: -->
<!--      toc: yes -->
<!--  --- -->

---
title: Nineteen Years Later
author: Harry Potter
date: July 31, 2016
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---
___________________________________________________________________________________________________________


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='img/', echo=TRUE, warning=FALSE, message=FALSE,
                      digits = 3)
Sys.setenv(TZ="Europe/Berlin")
options(tz="Europe/Berlin")
set.seed(2003)
library(caret)
```



## Assessing the performance of a  regression model 

For regression problems there are not so much complexity for the assessment the overall performance of a model. Usually the following metrics are enough to have a good idea how good or bad your model is performing. They als os give a way to comapre different model and choose the better one. 

### Residual sum-of-squares (RSS)

$$ RSS = \sum_{i=1}^{n}(y_i - \hat{y_i})^2$$

$y_i$ indicates the actual realisation of the label/target in the test set and $ \hat{y_i}$ indicates a prediction of $y_i$ according to the  regression model, may be a Random Forest, Gradient Boosting or simply a lionear regression model.The hat symbol denotes the predicted  value.

### Residual Standard Error (RSE)

$$ RSS = \sqrt{\frac{1}{n-p-1}\times RSS}= \sqrt{\frac{1}{n-1}\times \sum_{i=1}^{n}(y_i - \hat{y_i})^2} $$

$p$ is the number of features/predictors 

### R-squared or fraction of variance explained ($R^2$)

$$R^2 = \frac{TSS-RSS}{TSS} = 1 - \frac{RSS}{TSS}$$

where $TSS =  \sum_{i=1}^{n}(y_i-\bar{y})^2$ is the total sum of squares and $\bar{y}$ indicates the mean of $y_1, y_2, \cdots, y_n$  


## Assessing the the performance of a classification model  

Choosing the right evaluation metric for classification models is important to the success of a machine learning application. Monitoring only the `accuracy` score gives an incomplete picture of the performance of the model and can impact the effectiveness. In this lesson we will go through a number of technical evaluation metrics and some other business related metrics. 

### Data 
To show the use of these evaluation metrics, we need a classification model. Our example is based on a publicly available dataset, called the `Bank Marketing` Data Set. The data set comes from a Portugese bank and deals with a frequently-posed marketing question: whether a customer did or did not acquire a term deposit, a financial product. We will use `bank-additional-full.csv` dataset. It contains the information of 41.188 customers and 21 columns of information. 

### Packages 

Later in this lesson we will use `caret` package to train a number of  classification models. The `caret` package (short for Classification And REgression Training) contains functions to streamline the model training process for complex regression and classification problems. The package utilizes a number of `R` packages but tries not to load them all at package start-up. The package "suggests" field or options includes more than 30 packages. including `glm` for generalized linear models `randomForest` and `gmb`. The reason we are using  `caret` in this lesson is that we will be using another package named `modelplotr` which needs inputs objects generated by `caret` package  or `mlr` package. Anyway if you are planning to use `R` for machine learning modelling you should also learn about these two packages.  

The main help pages for the `caret` are at this [GitBook](https://topepo.github.io/caret/). Here, there are extended examples and a large amount of information that previously found in the package vignettes.

For `mlr` you may want to take look at the [official website](https://mlr.mlr-org.com/articles/tutorial/task.html) of this package.
      
Next to `caret`we will use [InformationValue](https://cran.r-project.org/web/packages/InformationValue/vignettes/InformationValue.html)




### Technical evaluation metrics  

To illustrate how to use [InformationValue](https://cran.r-project.org/web/packages/InformationValue/vignettes/InformationValue.html) and [modelplotr](https://modelplot.github.io/intro_modelplotr.html), let's say that we work for this bank and our marketing colleagues have asked us to help to select the customers that are most likely to respond to a term deposit offer. For that purpose, we will develop a predictive model and create the plots to discuss the results with our marketing colleagues. Since we want to show you how to build the plots, not how to build a perfect model, we'll use six of these columns in our example. Here's a short description on the data we use:

* _y_: has the client subscribed a term deposit?

* _duration_: last contact duration, in seconds (numeric)

* _campaign_: number of contacts performed during this campaign and for this client

* _pdays:_ number of days that passed by after the client was last contacted from a previous campaign

* _previous_: number of contacts performed before this campaign and for this client (numeric)

* _euribor3m_: euribor 3 month rate


```{r}
bank <- read.table("./data/bank-additional-full.csv", header = TRUE, sep = ";", stringsAsFactors = FALSE) 
bank <- bank[,c('y','duration','campaign','pdays','previous','euribor3m')]

# rename target class value 'yes' for better interpretation
bank$y[bank$y=='yes'] <- 'term.deposit'

#explore data
str(bank)
```

#### Data splitting 

The data set needs to be divided into training and test sets. Model training is performed using the training set and test set is used to assess the performance of the classification model. In our case, we will split the data with random sampling from the complete daat set. We will sample `70%` of the data set as training, `30%` of it as a test set.

```{r}
# prepare data for training and train models 
 test_size = 0.3
 train_index =  sample(seq(1, nrow(bank)),size = (1 - test_size)*nrow(bank) ,replace = F)
 train <- bank[train_index,]
 test <- bank[-train_index,]
```

#### Fitting a logistic model 

```{r}
# setting caret cross validation, here tuned for speed (not accuracy!)

fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)

logitmod   <- caret::train(y ~.,data = train, method = "glmnet",family="binomial",trControl = fitControl)
# logitmod  <- glm(factor(y) ~.,data = train, family = "binomial")

# random forest using randomForerst package, here tuned for speed (not accuracy!)
#rfmod  = caret::train(y ~.,data = train, method = "rf", trControl = fitControl, tuneGrid = expand.grid(.mtry = 2)) 

# Gradient boosting machine using gbm package, here tuned for speed  
#gbmod  = caret::train(y ~.,data = train, method = "gbm",distribution="bernoulli", verbose=F,trControl = fitControl, tuneGrid=data.frame(.n.trees= 1000, .shrinkage=0.01,.interaction.depth=1,.n.minobsinnode=1)) 

```



#### The Confusion Matrix

The caret package provides the awesome confusionMatrix function for this. It takes in the predicted and actual values. And to avoid confusion, always specify the positive argument. Otherwise, it is possible for `no` to be taken as `positive` or the `event`, and will cause a big mistake which may go unnoticed.
We want `term.deposit` Class to be `positive` so, I have set _"positive = term.deposit"_ below.


```{r}

logiPred = predict(logitmod, test, na.action = na.pass)
logiPred_prob = predict(logitmod, test, type = "prob")
postResample(logiPred, factor(test$y))
df <-  cbind(test$y,logiPred_prob)
predictions <- df[,c(1,3)] 
names(predictions) <- c("deposit", "pred")
(confmat <- caret::confusionMatrix(logiPred, factor(test$y), positive= "term.deposit", mode = "everything"))
tbl <- table(logiPred, factor(test$y))

```




In the above output, the table in the first 4 lines of the output is the confusion matrix. The remaining part of the output shows a bunch of more valuable evaluation metrics. Let's break down what they are

#### How to interpret confusionMatrix?
First, let's focus on the first 4 lines of the above output. 


The rows in the confusion matrix are the count of predicted `no s` and `term.deposits` (from `logiPred`), while, the columns are the actuals (from `test$y`). So, we have  `r confmat$table[1,1]` out of `r confmat$table[1,1]+confmat$table[2,1]` = `r confmat$table[1,1]` + `r confmat$table[2,1]` __not a term deposits__ observations predicted as __not a term deposits__ and `r confmat$table[2,2]` out of `r confmat$table[1,2] + confmat$table[2,2]`  __term.deposit__ observations predicted as __term.deposit__.

Secondly, look at the `r confmat$table[1,2]` in top-right of the table. This means the model predicted `r confmat$table[1,2]` instance as __not a term deposit__  which was actually __term.deposit__. This is a classic case of `False Negative` or `Type II error`. You want to avoid this at all costs, especially in medecine application, because, it says the patient is healthy when he is actually carrying a disease.

Also, the model predicted 263 observations as __term.deposit__ when the custgomer has actually __not a term deposit__. This is called `False Positive` or Type I error. This condition should also be avoided but in this case is not as dangerous as `Type II error`.

```{r pressure, echo=FALSE, fig.cap="False positive vs false negative",  fig.align='center', out.width = '80%'}
knitr::include_graphics("img/false_pos_pregnant.png")
```


#### What is Sensitivity, Specificity and Detection Rate?

_Sensitivity_ is the percentage of actual __term.deposit__ that were correctly predicted. It shows what percentage of __term.deposit__ were covered by the model.

The total number of __term.deposit__ is  `r sum(confmat$table[1,2]+confmat$table[2,2])` out of which `r confmat$table[2,2]` was correctly predicted. So, sensitivity is `r confmat$table[2,2]`/`r sum(confmat$table[1,2]+confmat$table[2,2])` = `r confmat$table[2,2]/sum(confmat$table[1,2]+confmat$table[2,2])`. 

Sensitivity matters more when classifying the positives correctly is more important than classifying the negatives.

Likewise, _Specificity_ is the proportion of actual __not a term deposits__ that were correctly predicted. So in this case, it is
`r confmat$table[1,1]`/`r sum(confmat$table[1,1] + confmat$table[2,1])` = `r confmat$table[1,1]/sum(confmat$table[1,1]+confmat$table[2,1])`. 

Specificity matters more when classifying the `negatives` correctly is more important than classifying the `positives`. Maximizing specificity is more relevant in cases like spam detection, where you strictly don't want genuine messages `no spam` to end up in `spam`.

_Detection rate_ is the proportion of the whole sample where the positives were detected correctly. So, it is

`r confmat$table[2,2]`/(`r confmat$table[1,1]`+ `r confmat$table[1,2]` + `r confmat$table[2,1]` + `r confmat$table[2,2]`) = 
`r confmat$table[2,2]/sum(confmat$table[1,1]+confmat$table[1,2]+confmat$table[2,1]+confmat$table[2,2])`

You can see further explanation of all the metrics in this [wiki link](https://en.wikipedia.org/wiki/Confusion_matrix)

#### What is Precision, Recall and F1 Score?


Another great way to know the goodness of the model is using the `Precision`, `Recall` and the `F1 Score`. The approach here is to find what percentage of the model's positives predictions are accurate. This is nothing but `Precision.`

Let's suppose you have a model with high precision, we also want to know what percentage of ALL poesitives were covered. This can be captured using `Sensitivity`.

But in this context, it is known as `Recall`. Just because, it is customary to call them together as `Precision and Recall`.

A high precision score gives more confidence to the model's capability to classify the positives. Combining this with Recall gives an idea of how many of the total positives it was able to cover.

A good model should have a good precision as well as a high recall. So ideally, I want to have a measure that combines both these aspects in one single metric - `the F1 Score`.

$$
F1 \thinspace Score = \frac{2 \times Precision \times Recall}{Precision + Recall}
$$
These two first metrics can be computed using the `caret` package. For the `F1 score`we compute it using the previous formula

```{r}
#precision(tbl)
(prec <- caret::precision(data = logiPred, reference = factor(test$y), relevant = "term.deposit"))
#logiPred, factor(test$y)
#recall(tbl)
(rec <- caret::recall(data = logiPred, reference = factor(test$y), relevant = "term.deposit"))

(fscore <- (2*prec*rec)/(prec+rec))

```






#### What is Cohen's Kappa?

Kappa is similar to Accuracy score, but it takes into account the accuracy that would have happened anyway through random predictions.

$$
Kappa = (Observed \ Accuracy - Expected \ Accuracy) / (1 - Expected \ Accuracy)
$$

Cohen's kappa is shown as an output of caret's confusionMatrix function.


#### The ROC Curve?

Often, choosing the best model is sort of a balance between predicting the positive accurately or the negative accurately. In other words sensitivity and specificity.

But it would be great to have something that captures both these aspects in one single metric. This is nicely captured by the `Receiver Operating Characteristics` curve, also called as the `ROC` curve. In fact, the area under the ROC curve can be used as an evaluation metric to compare the efficacy of the models.

The `ROC` curve is the interpolated curve made of points whose coordinates are functions of the threshold.

$$
ROC_x(\theta) = FPR(\theta) = \frac{FP(\theta)}{FP(\theta)+ TN(\theta)} = \frac{FP(\theta)}{Total \ Negatives } = Sensitivity\\
ROC_y(\theta) = TPR(\theta) = \frac{TP(\theta)}{FN(\theta)+ TP(\theta)} = 1 - \frac{TP(\theta)}{Total \ Positives} = 1- Specifity
$$




```{r}
source("calculate_roc.R")
source("plot_roc.R")
roc <- calculate_roc(predictions, 1, 2, n = 100)
plot_roc(roc, 0.5, 1, 3)
```

```{r}
library(pROC)
resRoc <- pROC::roc(predictions$deposit, predictions$pred)
plot(resRoc, legacy.axes = TRUE)
auc(predictions$deposit, predictions$pred)
```

#### What is KS Statistic and How to interpret KS Chart

K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives. On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases. So how to compute the Kolmogorov-Smirnov statistic?

* Once the prediction probability scores are obtained, the observations are sorted by decreasing order of probability scores. This way, you can expect the rows at the top to be classified as `positive` while rows at the bottom to be `negative`.

* All observations are then split into 10 equal sized buckets (bins or deciles).

* Then, `KS statistic` is the maximum difference between the cumulative true positive and cumulative false positive rate. The `KS statistic` can be computed using the `ks_stat` function in `InformationValue` package. By setting the `returnKSTable = T`, you can retrieve the table that contains the detailed decile level splits.

```{r}

library(InformationValue)
# ks_stat(as.numeric(factor(test$y)),as.numeric(logiPred))
(ksstat <- ks_stat(factor(test$y),logiPred, returnKSTable = T))
```

#### How to plot Kolmogorov Smirnov Chart in R?

The `KS Chart` is particularly useful in marketing campaigns and ads click predictions where you want to know the right population size to target to get the maximum response rate.

The `KS Chart` below shows how this might look like. The length of the vertical dashed red line indicates the `KS Statistic`


```{r}
# ks_plot(factor(test$y),logiPred)
```

See here for the ks_plot R code to reproduce the kolmogorov-smirnov chart.
By targeting the top 40% of the population (point it touches the X-axis), the model is able to cover 97.18% of responders (1's).

The KS chart and statistic that is widely used in credit scoring scenarios and for selecting the optimal population size of target users for marketing campaigns.




## Old wine in new bottles: a business perspective


```{r techroc, echo=FALSE, fig.cap="",  fig.align='center', out.width = '80%'}

knitr::include_graphics("img/roc_perf01.png")

```

Before we throw more code and output at you, let's get you familiar with more friendly plots that are more intuitive for business colleagues. Although each plot sheds light on the business value of your model from a different angle, they all use the same data:

* Predicted probability for the target class

* Equally sized groups based on this predicted probability

* Actual number of observed target class observations in these groups

It's common practice to split the data to score into 10 equally large groups and call these groups deciles (we already saw this technique with the `KS statistic`). Observations that belong to the `top-10%` with highest model probability in a set, are in decile 1 of that set; the next group of `10%` with high model probability are decile 2 and finally the `10%` observations with the lowest model probability on the target class belong to decile 10.

Each of our four plots places the deciles on the x axis and another measure on the y axis. The deciles are plotted from left to right so the observations with the highest model probability are on the left side of the plot. This results in plots like this




```{r modelper01, echo=FALSE, fig.cap="",  fig.align='center', out.width = '80%'}
knitr::include_graphics("img/model_per01.png")
```

Now that it's clear what is on the horizontal axis of each of the plots, we can go into more detail on the metrics for each plot on the vertical axis. For each plot, we'll start with a brief explanation what insight you gain with the plot from a business perspective. After that, we apply it to our banking data and show some neat features of modelplotr to help you explain the value of your wonderful predictive models to others.

### Cumulative gains plot

The cumulative gains plot - often named 'gains plot' - helps you answer the question:

> When we apply the model and select the best `X` deciles, what `%` of the actual target class observations can we expect to target?

Hence, the cumulative gains plot visualises the percentage of the target class members you have selected if you would decide to select up until decile `X`. This is a very important business question, because in most cases, you want to use a predictive model to target a subset of observations - customers, prospects, cases, etc. instead of targeting all cases. And since we won't build perfect models all the time, we will miss some potential. And that's perfectly all right, because if we are not willing to accept that, we should not use a model in the first place. Or build a perfect model, that scores all actual target class members with a `100%` probability and all the cases that do not belong to the target class with a `0%` probability.

So, we'll have to accept we will lose some. _What percentage_ of the actual target class members you do select with your model at a given decile, that is what the cumulative gains plot tells you. The plot comes with two reference lines to tell you how good/bad your model is doing: The random model line and the wizard model line. The random model line tells you what proportion of the actual target class you would expect to select when no model is used at all. This vertical line runs from the origin (with `0%` of cases, you can only have `0%` of the actual target class members) to the upper right corner (with `100%` of the cases, you have `100%` of the target class members). It's the rock bottom of how your model can perform; are you close to this, then your model is not much better than a coin flip. The wizard model is the upper bound of what your model can do. It starts in the origin and rises as steep as possible towards `100%`. If less than `10%` of all cases belong to the target category, this means that it goes steep up from the origin to the value of decile 1 and cumulative gains of `100%` and remains there for all other deciles as it is a cumulative measure. Your model will always move between these two reference lines - closer to a wizard is always better - and looks like this:

```{r modelper02, echo=FALSE, fig.cap="",  fig.align='center', out.width = '80%'}
knitr::include_graphics("img/model_per02.png")
```

Back to our business example. We will be using the  `modelplotr` package. More on this in the `modelplotr` package documentation, just have a look at `?modelplotr::aggregate_over_deciles()`. We first create a few models to evaluate with our plots.
```{r}
# setting caret cross validation, here tuned for speed (not accuracy!)

fitControl <- caret::trainControl(method = "cv",number = 2,classProbs=TRUE)

# Fit a logistic model
logitmod  = caret::train(y ~.,data = train, method = "glmnet",family="binomial",trControl = fitControl)

# random forest using randomForerst package, here tuned for speed (not accuracy!)
rfmod  = caret::train(y ~.,data = train, method = "rf", trControl = fitControl, tuneGrid = expand.grid(.mtry = 2))

# Gradient boosting machine using gbm package, here tuned for speed
gbmod  = caret::train(y ~.,data = train, method = "gbm",distribution="bernoulli", verbose=F,trControl = fitControl, tuneGrid=data.frame(.n.trees= 1000, .shrinkage=0.01,.interaction.depth=1,.n.minobsinnode=1))

```


Ok, we've generated some predictive models. Now, let's prepare the data for plotting! We will focus on explaining to our marketing colleagues how good our predictive model works and how it can help them select customers for their term deposit campaign.

```{r}
library(modelplotr)
# transform datasets and model objects into scored data and calculate deciles
prepare_scores_and_deciles(datasets=list("train","test"),
  dataset_labels = list("train data","test data"),
  models = list("rfmod","logitmod","gbmod"),
  model_labels = list("Random forest","Logitic regression","Boosting gradient machine"),
  target_column="y")


# transform data generated with prepare_scores_and_deciles into aggregated data for chosen plotting scope
# plotting_scope(select_model_label = 'Gradient boosting machine',select_dataset_label = 'test data')
plotting_scope(select_model_label = 'Boosting gradient machine',select_dataset_label = 'test data')
#
```



How many of the term deposit buyers can we select with the `top-20%` of our predictive models? Let's find out! To generate the cumulate gains plot, we can simply call the function `plot_cumgains():`

```{r}
# plot the cumulative gains plot
plot_cumgains()
```

We don't need to specify any parameters, since the default input is plot_input, which is generated with the `plotting_scope()` function we ran previously. There are several parameters available to customize the plot, though. If we want to emphasize the model performance at a given point, we can add both highlighting to the plot and add some explanatory text below the plot. Both are optional, though:

```{r}
# plot the cumulative gains plot and annotate the plot at decile = 2
plot_cumgains(highlight_decile = 5)
```

Our _highlight_decile_ parameter adds some guiding elements to the plot at decile 2 as well as a text box below the graph with the interpretation of the plot at decile 2 in words. This interpretation is also printed to the console. Our simple model - only 6 pedictors were used - seems to do a nice job selecting the customers interested in buying term deposites. When we select 20% with the highest probability according to `gbm`, this selection holds 82% of all term deposit cases in test data. With a perfect model, we would have selected `100%`, since less than `20%` of all customers in the test set buy term deposits. A random pick would only hold `20%` of the customers with term deposits. How much better than random we do, brings us to plot number 2!


### Cumulative lift plot

The cumulative lift plot, often referred to as lift plot or index plot, helps you answer the question:

> When we apply the model and select the best `X` deciles, how many times better is that than using no model at all?

The lift plot helps you in explaining how much better selecting based on your model is compared to taking random selections instead. Especially when models are not yet used within a certain organisation or domain, this really helps business understand what selecting based on models can do for them.

The lift plot only has one reference line: the `random model`. With a random model we mean that each observation gets a random number and all cases are devided into deciles based on these random numbers. The `%` of actual target category observations in each decile would be equal to the overall `%` of actual target category observations in the total set. Since the lift is calculated as the ratio of these two numbers, we get a horizontal line at the value of `1`. Your model should however be able to do better, resulting in a high ratio for decile 1. How high the lift can get, depends on the quality of your model, but also on the `%` of target class observations in the data: If `50%` of your data belongs to the target class of interest, a perfect model would only do twice as good (lift: 2) as a random selection. With a smaller target class value, say `10%`, the model can potentially be `10` times better (lift: 10) than a random selection. Therefore, no general guideline of a 'good' lift can be specified. Towards decile 10, since the plot is cumulative, with 100% of cases, we have the whole set again and therefore the cumulative lift will always end up at a value of 1. It looks like this:

```{r modelper03, echo=FALSE, fig.cap="",  fig.align='center', out.width = '80%'}
knitr::include_graphics("img/model_per03.png")
```


To generate the cumulative lift plot for our `gbm` odel predicting term deposit buying, we call the function `plot_cumlift()`. Let's add some highlighting to see how much better a selection based on our model containing deciles 1 and 2 would be, compared to a random selection of `20%` of all customers:

```{r}
# plot the cumulative lift plot and annotate the plot at decile = 2
plot_cumlift(highlight_decile = 1)
```



A term deposit campaign targeted at a selection of `20%` of all customers based on the gradien boosting  model can be expected to have a 4 times higher response (`408%`) compared to a random sample of customers. Not bad, right? The cumulative lift really helps in getting a positive return on marketing investments. It should be noted, though, that since the cumulative lift plot is relative, it doesn't tell us how high the actual reponse will be on our campaign.


### Response plot

One of the easiest to explain evaluation plots is the response plot. It simply plots the percentage of target class observations per decile. It can be used to answer the following business question:

> When we apply the model and select decile `X`, what is the expected `%` of target class observations in that decile?

The plot has one reference line: The `%` of target class cases in the total set. It looks like this:

```{r modelper04, echo=FALSE, fig.cap="",  fig.align='center', out.width = '80%'}
knitr::include_graphics("img/model_per04.png")
```



A good model starts with a high response value in the first decile(s) and suddenly drops quickly towards 0 for later deciles. This indicates good differentiation between target class members - getting high model scores - and all other cases. An interesting point in the plot is the location where your model's line intersects the random model line. From that decile onwards, the % of target class cases is lower than a random selection of cases would hold.

To generate the response plot for our term deposit model, we can simply call the function plot_response(). Let's immediately highlight the plot to have the interpretation of the response plot at decile 1 added to the plot:

```{r}
# plot the response plot and annotate the plot at decile = 1
plot_response(highlight_decile = 1)
```



As the plot shows and the text below the plot states: When we select decile 1 according to model Gradient Boosting model in dataset test data the `%` of term deposit cases in the selection is `63%`. This is quite good, especially when compared to the overall likelihood of `11%`. The response in the second decile is much lower, about `29%`. From decile 3 onwards, the expected response will be lower than the overall likelihood of 10.4%. However, most of the time, our model will be used to select the highest decile up until some decile. That makes it even more relevant to have a look at the cumulative version of the response plot.



### Cumulative response plot

Finally, one of the most used plots: The cumulative response plot. It answers the question burning on each business reps lips:

When we apply the model and select up until decile X, what is the expected % of target class observations in the selection?

The reference line in this plot is the same as in the response plot: the % of target class cases in the total set.


```{r modelper05, echo=FALSE, fig.cap="",  fig.align='center', out.width = '80%'}
knitr::include_graphics("img/model_per05.png")
```

Whereas the response plot crosses the reference line, in the cumulative response plot it never crosses it but ends up at the same point for decile 10: Selecting all cases up until decile 10 is the same as selecting all cases, hence the % of target class cases will be exactly the same. This plot is most often used to decide - together with business colleagues - up until what decile to select for a campaign.

Back to our banking business case. To generate the cumulative response plot, we call the function `plot_cumresponse()`. Let's highlight it at decile 3 to see what the overall expected response will be if we select prospects for a term deposit offer based on our gradient boosting machine model:

```{r}
# plot the cumulative response plot and annotate the plot at decile = 3
plot_cumresponse(highlight_decile = 3)
```

When we select deciles 1 until 3 according to the gradient boosting machine model in dataset test data the `%` of term deposit cases in the selection is `35%`. Since the test data is an independent set, not used to train the model, we can expect the response on the term deposit campaign to be `35%`.

The cumulative response percentage at a given decile is a number your business colleagues can really work with: Is that response big enough to have a successfull campaign, given costs and other expectations? Will the absolute number of sold term deposits meet the targets? Or do we lose too much of all potential term deposit buyers by only selecting the top `30%`? To answer that question, we can go back to the cumulative gains plot. And that's why there's no absolute winner among these plots and we advice to use them all. To make that happen, there's also a function to easily combine all four plots.


### All four plots together
With the function call plot_all we get all four plots on one grid. We can easily save it to a file to include it in a presentation or share it with colleagues.

```{r}
# plot all four evaluation plots and save to file
plot_all(save_fig = TRUE,save_fig_filename = 'Selection model Term Deposits')
##
```



With these plots, we are able to talk with business colleagues about the actual value a predictive model, without having to bore them with technicalities. The `modelplotr` package translated the preformance metrics of the  model in business terms and visualised it to simplify interpretation and communication. 


#### Get more out of modelplotr: using different scopes.
As we mentioned earlier, the `modelplotr` package also enables to make interesting comparisons, using the scope parameter. Comparisons between different models, between different datasets and (in case of a multiclass target) between different target classes. Curious? Please have a look at the package documentation or read our other posts on modelplotr.

However, to give one example, we could compare whether XGBoost was indeed the best choice to select the top-30% customers for a term deposit offer:

```{r}
# set plotting scope to model comparison
plotting_scope(scope = "compare_models")


# plot the cumulative response plot and annotate the plot at decile = 3
plot_cumresponse(highlight_decile = 3)

```




```{r techroc2, echo=FALSE, fig.cap="",  fig.align='center', out.width = '80%'}

knitr::include_graphics("img/roc_perf02.png")

```